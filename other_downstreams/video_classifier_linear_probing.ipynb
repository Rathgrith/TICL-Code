{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Before running this code, you need to prepare json files for video scene classification datasets. The keys are filenames and the keys are class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: ['Agriculture & Rural', 'Artistic Spaces', 'Bars & Nightlife', 'Campus', 'Dining & Food Outlets', 'Elevators & Escalators&Stairs', 'Historic & Religious Sites', 'Hotel & Temporary Stay', 'Indoor Educational Spaces', 'Indoor Entertainment Venues', 'Indoor Residential Spaces', 'Indoor Shops & Retail& Commercial', 'Indoor sports venues', 'Kitchen', 'Nature', 'Open Public Spaces', 'Outdoor Commercial & Markets', 'Outdoor Residences & Living', 'Outdoor Sports & Athletic Fields', 'Outdoor Transportation', 'Parks & Recreational Areas', 'Public Gathering & Conference Spaces', 'Scientific interior space', 'Storage & Utility', 'Transportation Interiors', 'Urban Constructions & street', 'Waterfronts & Water Bodies', 'Workspaces'].\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorchvideo.data\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "import imageio\n",
    "import decord  # Added import statement\n",
    "DATA_ROOT = 'FILL_IN_YOUR_PATH'\n",
    "# Select dataset. Options: \"Hollywood2\", \"YUP++\", \"360x\"\n",
    "dataset = \"360x\"\n",
    "\n",
    "if dataset == \"Hollywood2\":\n",
    "    with open(f'{DATA_ROOT}/Hollywood2/hollywood2.json') as f:\n",
    "        hollywood2_data = json.load(f)\n",
    "\n",
    "    # Create label mappings\n",
    "    labels = set()\n",
    "    for v in hollywood2_data.values():\n",
    "        for k, v in v['label'].items():\n",
    "            if v:\n",
    "                labels.add(k)\n",
    "    labels = sorted(list(labels))\n",
    "    label2id = {label: i for i, label in enumerate(labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "    print(f\"Unique classes: {labels}.\")\n",
    "\n",
    "elif dataset == \"YUP++\":\n",
    "    with open(f'{DATA_ROOT}/YUP++/test.json') as f:\n",
    "        yup_data = json.load(f)\n",
    "    # simply read all the labels\n",
    "    labels = set()\n",
    "    for entry in yup_data:\n",
    "        labels.add(entry['label'])\n",
    "    labels = sorted(list(labels))\n",
    "    label2id = {label: i for i, label in enumerate(labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    print(f\"Unique classes: {labels}.\")\n",
    "elif dataset == \"360x\":\n",
    "    path = f'{DATA_ROOT}/360x/index.json'\n",
    "    with open(path) as f:\n",
    "        yup_data = json.load(f)\n",
    "    # simply read all the labels\n",
    "    labels = set()\n",
    "    for entry in yup_data:\n",
    "        temp_label = yup_data[entry]['label']\n",
    "        labels.add(temp_label)\n",
    "    labels = sorted(list(labels))\n",
    "    label2id = {label: i for i, label in enumerate(labels)}\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    print(f\"Unique classes: {labels}.\")\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {dataset}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([28]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([28, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames to sample: 16\n",
      "Sample rate: 4\n",
      "Clip duration: 2.1333333333333333 seconds\n",
      "Frame size: 224x224\n",
      "Mean: [0.485, 0.456, 0.406]\n",
      "Std: [0.229, 0.224, 0.225]\n",
      "Removed 1 entries.\n",
      "[{'binocular_files_number': 2, 'capture_time': '20231022T1130742', 'category': 'Dining & Food Outlets', 'weather': 'indoor', 'text_description': 'Eating in Five Guys', 'gps': [52.4827, -1.89761], 'video_name': 'd86992ee-abc2-4dc2-9617-17263257d201'}] removed\n",
      "Removed 1 entries.\n",
      "[{'binocular_files_number': 2, 'capture_time': '20231022T1130742', 'category': 'Dining & Food Outlets', 'weather': 'indoor', 'text_description': 'Eating in Five Guys', 'gps': [52.4827, -1.89761], 'video_name': 'd86992ee-abc2-4dc2-9617-17263257d201'}] removed\n",
      "Removed 0 entries.\n",
      "[] removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"MCG-NJU/videomae-base-finetuned-kinetics\"\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_checkpoint)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# Extract configurations\n",
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = 16\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "print(f\"Number of frames to sample: {num_frames_to_sample}\"\n",
    "      f\"\\nSample rate: {sample_rate}\"\n",
    "      f\"\\nClip duration: {clip_duration} seconds\"\n",
    "      f\"\\nFrame size: {height}x{width}\"\n",
    "      f\"\\nMean: {mean}\"\n",
    "      f\"\\nStd: {std}\")\n",
    "# Dataset transformations\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    # RandomShortSideScale(min_size=256, max_size=320),\n",
    "                    Resize(resize_to),\n",
    "                    RandomCrop(resize_to),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to),\n",
    "                    # CenterCrop(resize_to),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Hollywood2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.video_paths = [item[\"path\"] for item in self.data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_info = self.data[idx]\n",
    "    \n",
    "        video_path = os.path.join(f\"{DATA_ROOT}/Hollywood2/AVIClipsScenes/\", os.path.basename(self.video_paths[idx]))\n",
    "        label = [k for k, v in video_info[\"label\"].items() if v][0]\n",
    "        label_id = label2id[label]\n",
    "\n",
    "        # Load video\n",
    "        video = decord.VideoReader(video_path)\n",
    "        # Simple sampling method (take every nth frame, n = len(video) // num_frames_to_sample)\n",
    "        frames = [video[i].asnumpy() for i in range(0, len(video), max(1, len(video) // num_frames_to_sample))]\n",
    "\n",
    "        # Stack frames to form a tensor\n",
    "        video_tensor = torch.tensor(np.stack(frames))\n",
    "        if self.transform:\n",
    "            video_tensor = video_tensor.permute(3, 0, 1, 2)  # Convert (T, H, W, C) to (C, T, H, W)\n",
    "            video_tensor = self.transform({\"video\": video_tensor})[\"video\"]\n",
    "\n",
    "        return {\"video\": video_tensor, \"label\": label_id}\n",
    "\n",
    "class YUPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        # video path are ['path'] for each entries in the list\n",
    "        self.video_paths = [entry['path'] for entry in self.data]\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        # strip ./ from the path\n",
    "        video_path = os.path.join(f\"{DATA_ROOT}/YUP++/\", self.video_paths[idx].strip(\"./\"))\n",
    "        # video_path = os.path.join(\"./YUP++\", self.video_paths[idx])\n",
    "        #  self.data[self.video_paths[idx]][\"path\"]\n",
    "        label = self.data[idx]['label']\n",
    "        # print(label)\n",
    "        label_id = label2id[label]\n",
    "        video = decord.VideoReader(video_path)\n",
    "        frames = [video[i].asnumpy() for i in range(0, len(video), len(video) // num_frames_to_sample)]\n",
    "        video_tensor = torch.tensor(np.stack(frames))\n",
    "        # print(video_tensor.shape)\n",
    "        if self.transform:\n",
    "            video_tensor = video_tensor.permute(3, 0, 1, 2)  # Convert (T, H, W, C) to (C, T, H, W)\n",
    "            video_tensor = self.transform({\"video\": video_tensor})[\"video\"]\n",
    "\n",
    "        return {\"video\": video_tensor, \"label\": label_id}\n",
    "\n",
    "    \n",
    "class x360Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.video_paths = [item[\"video_name\"] for item in self.data]\n",
    "        self.labels = [item[\"category\"] for item in self.data]\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_info = self.data[idx]\n",
    "        temp_path = f'{DATA_ROOT}/x360/third_person/'\n",
    "        video_path = temp_path +'.mp4'\n",
    "        # print(os.path.exists(video_path))\n",
    "        label = self.labels[idx]\n",
    "        label_id = label2id[label]\n",
    "\n",
    "        # Load video\n",
    "        video = decord.VideoReader(video_path)\n",
    "        # Simple sampling method (take every nth frame, n = len(video) // num_frames_to_sample)\n",
    "        frames = [video[i].asnumpy() for i in range(0, len(video), max(1, len(video) // num_frames_to_sample))]\n",
    "        #print(frames[1].shape)\n",
    "        # Stack frames to form a tensor\n",
    "        video_tensor = torch.tensor(np.stack(frames),dtype=torch.uint8)\n",
    "        if self.transform:\n",
    "            video_tensor = video_tensor.permute(3, 0, 1, 2)  # Convert (T, H, W, C) to (C, T, H, W)\n",
    "            video_tensor = self.transform({\"video\": video_tensor})[\"video\"]\n",
    "\n",
    "        return {\"video\": video_tensor, \"label\": label_id}\n",
    "    \n",
    "# Load the data and ensure it is in the required structure\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        data = list(data.values())\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_x360_data(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    if isinstance(data, dict):\n",
    "        data = list(data.values())\n",
    "    return data\n",
    "\n",
    "# Load data\n",
    "if dataset == \"Hollywood2\":\n",
    "    train_data = load_data(f'{DATA_ROOT}/Hollywood2/train_split_metadata.json')\n",
    "    val_data = load_data(f'{DATA_ROOT}/Hollywood2/val_split_metadata.json')\n",
    "    test_data = load_data(f'{DATA_ROOT}/Hollywood2/test_data.json')\n",
    "    train_dataset = Hollywood2Dataset(data=train_data, transform=train_transform)\n",
    "    val_dataset = Hollywood2Dataset(data=val_data, transform=val_transform)\n",
    "    test_dataset = Hollywood2Dataset(data=test_data, transform=val_transform)\n",
    "\n",
    "elif dataset == \"YUP++\":\n",
    "    yup_test_data = load_data(f\"{DATA_ROOT}/YUP++/yup_test_data.json\")\n",
    "    yup_val_data = load_data(f\"{DATA_ROOT}/YUP++/yup_val_data.json\")\n",
    "    yup_train_data = load_data(f\"{DATA_ROOT}/YUP++/yup_train_data.json\")\n",
    "    train_dataset = YUPDataset(data=yup_train_data, transform=train_transform)\n",
    "    val_dataset = YUPDataset(data=yup_val_data, transform=val_transform)\n",
    "    test_dataset = YUPDataset(data=yup_test_data, transform=val_transform)\n",
    "elif dataset == \"360x\":\n",
    "    test_data = load_x360_data(f'{DATA_ROOT}/360x_test_data.json')\n",
    "    val_data = load_x360_data(f'{DATA_ROOT}/360x_test_data.json')\n",
    "    train_data = load_x360_data(f'{DATA_ROOT}/360x_train_data.json')\n",
    "    train_dataset = x360Dataset(data=train_data, transform=train_transform)\n",
    "    val_dataset = x360Dataset(data=val_data, transform=val_transform)\n",
    "    test_dataset = x360Dataset(data=test_data, transform=val_transform)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# Visualization function\n",
    "def unnormalize_img(img):\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "    \n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n",
    "\n",
    "sample_video = next(iter(train_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video = next(iter(val_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "# display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video = next(iter(test_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "# display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "from torchvision.transforms import ToPILImage\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from torchvision import models\n",
    "import timm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='huggingface_hub.*')\n",
    "\n",
    "class PureImageEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PureImageEncoder, self).__init__()\n",
    "        self.CLIP = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.image_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        c_in = 768\n",
    "        reduction = 4\n",
    "\n",
    "        # Freeze CLIP\n",
    "        for param in self.CLIP.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def preprocess_image(self, image):\n",
    "        x = self.image_processor(images=image, return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # if input is pre-computed features of CLIP, skip the get_features step\n",
    "        if x.shape[-1] == 768:\n",
    "            pass\n",
    "        else:\n",
    "            x = self.CLIP.get_image_features(pixel_values=x)\n",
    "        return x\n",
    "\n",
    "from ..model.TICL import TICL\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                       ._ o o                                               \n",
      "                                       \\_`-)|_\n",
      "                                    ,\"\"       \\ \n",
      "                                  ,\"  ## |   ಠ ಠ. \n",
      "                                ,\" ##   ,-\\__    `.\n",
      "                              ,\"       /     `--._;)      ///// Please wait, and...\n",
      "                            ,\"     ## /\n",
      "                          ,\"   ##    /\n",
      " _____ _               _                                                                    _                  _          _               \n",
      "/  __ \\ |             | |                                                                  | |                | |        | |              \n",
      "| /  \\/ |__   ___  ___| | __  _   _  ___  _   _ _ __   _ __   __ _ _ __ __ _ _ __ ___   ___| |_ ___ _ __ ___  | |__   ___| | _____      __\n",
      "| |   | '_ \\ / _ \\/ __| |/ / | | | |/ _ \\| | | | '__| | '_ \\ / _` | '__/ _` | '_ ` _ \\ / _ \\ __/ _ \\ '__/ __| | '_ \\ / _ \\ |/ _ \\ \\ /\\ / /\n",
      "| \\__/\\ | | |  __/ (__|   <  | |_| | (_) | |_| | |    | |_) | (_| | | | (_| | | | | | |  __/ ||  __/ |  \\__ \\ | |_) |  __/ | (_) \\ V  V / \n",
      " \\____/_| |_|\\___|\\___|_|\\_\\  \\__, |\\___/ \\__,_|_|    | .__/ \\__,_|_|  \\__,_|_| |_| |_|\\___|\\__\\___|_|  |___/ |_.__/ \\___|_|\\___/ \\_/\\_/  \n",
      "                               __/ |                  | |                                                                                 \n",
      "                              |___/                   |_|                                                                                 \n",
      "\n",
      "                    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base-finetuned-kinetics and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([28]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([28, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomVideoMAE(\n",
      "  (videomae): VideoMAEModel(\n",
      "    (embeddings): VideoMAEEmbeddings(\n",
      "      (patch_embeddings): VideoMAEPatchEmbeddings(\n",
      "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
      "      )\n",
      "    )\n",
      "    (encoder): VideoMAEEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x VideoMAELayer(\n",
      "          (attention): VideoMAEAttention(\n",
      "            (attention): VideoMAESelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "            (output): VideoMAESelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): VideoMAEIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): VideoMAEOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (feature_extractor): ImageEncoder(\n",
      "    (CLIP): CLIPModel(\n",
      "      (text_model): CLIPTextTransformer(\n",
      "        (embeddings): CLIPTextEmbeddings(\n",
      "          (token_embedding): Embedding(49408, 768)\n",
      "          (position_embedding): Embedding(77, 768)\n",
      "        )\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-11): 12 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (vision_model): CLIPVisionTransformer(\n",
      "        (embeddings): CLIPVisionEmbeddings(\n",
      "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "          (position_embedding): Embedding(257, 1024)\n",
      "        )\n",
      "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder): CLIPEncoder(\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x CLIPEncoderLayer(\n",
      "              (self_attn): CLIPAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): CLIPMLP(\n",
      "                (activation_fn): QuickGELUActivation()\n",
      "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              )\n",
      "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
      "      (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
      "    )\n",
      "    (adapter): Sequential(\n",
      "      (0): Linear(in_features=768, out_features=192, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=192, out_features=768, bias=False)\n",
      "      (3): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=28, bias=True)\n",
      "  (conv1x1): Conv2d(4, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////Run 360x_videomae1:5val_TEST_resizeT_bs2_epochs20_steps1520_lr7e-05_wr0.0_Timefeature=4frame_True_LP_third_view preparation finished...////\n",
      "                              _- _ , - . _\n",
      "                            `,% o` ~~-_,'.'\n",
      "                            % %@ - % %, -'%,\n",
      "                           ,-, . _ --\\ -.%\n",
      "                  P^=.     `'\"   |+|'    `\n",
      "                  ||             |+|\n",
      "                  ||             |+|\n",
      "                  ||             |+|\n",
      "            ______/|             |+|\n",
      "           `| ___ ,/             |+|\n",
      "            ||   ||              |+|\n",
      "            ||   ||              |+|\n",
      "    ________||___||___.__________/H|____\n",
      "    ______               _          __             _____         _       _             _ \n",
      "    | ___ \\             | |        / _|           |_   _|       (_)     (_)           | |\n",
      "    | |_/ /___  __ _  __| |_   _  | |_ ___  _ __    | |_ __ __ _ _ _ __  _ _ __   __ _| |\n",
      "    |    // _ \\/ _` |/ _` | | | | |  _/ _ \\| '__|   | | '__/ _` | | '_ \\| | '_ \\ / _` | |\n",
      "    | |\\ \\  __/ (_| | (_| | |_| | | || (_) | |      | | | | (_| | | | | | | | | | (_| |_|\n",
      "    \\_| \\_\\___|\\__,_|\\__,_|\\__, | |_| \\___/|_|      \\_/_|  \\__,_|_|_| |_|_|_| |_|\\__, (_)\n",
      "                            __/ |                                                 __/ |  \n",
      "                           |___/                                                 |___/                                                                                                                                                                                                                                                              \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Custom branch in the model\n",
    "# add an additional branch to the model\n",
    "# it should override class transformers.VideoMAEForVideoClassification\n",
    "# Custom branch in the model\n",
    "print(\"\"\"\\\n",
    "\n",
    "                                       ._ o o                                               \n",
    "                                       \\_`-)|_\n",
    "                                    ,\"\"       \\ \n",
    "                                  ,\"  ## |   ಠ ಠ. \n",
    "                                ,\" ##   ,-\\__    `.\n",
    "                              ,\"       /     `--._;)      ///// Please wait, and...\n",
    "                            ,\"     ## /\n",
    "                          ,\"   ##    /\n",
    " _____ _               _                                                                    _                  _          _               \n",
    "/  __ \\ |             | |                                                                  | |                | |        | |              \n",
    "| /  \\/ |__   ___  ___| | __  _   _  ___  _   _ _ __   _ __   __ _ _ __ __ _ _ __ ___   ___| |_ ___ _ __ ___  | |__   ___| | _____      __\n",
    "| |   | '_ \\ / _ \\/ __| |/ / | | | |/ _ \\| | | | '__| | '_ \\ / _` | '__/ _` | '_ ` _ \\ / _ \\ __/ _ \\ '__/ __| | '_ \\ / _ \\ |/ _ \\ \\ /\\ / /\n",
    "| \\__/\\ | | |  __/ (__|   <  | |_| | (_) | |_| | |    | |_) | (_| | | | (_| | | | | | |  __/ ||  __/ |  \\__ \\ | |_) |  __/ | (_) \\ V  V / \n",
    " \\____/_| |_|\\___|\\___|_|\\_\\  \\__, |\\___/ \\__,_|_|    | .__/ \\__,_|_|  \\__,_|_| |_| |_|\\___|\\__\\___|_|  |___/ |_.__/ \\___|_|\\___/ \\_/\\_/  \n",
    "                               __/ |                  | |                                                                                 \n",
    "                              |___/                   |_|                                                                                 \n",
    "\n",
    "                    \"\"\")\n",
    "from transformers import VideoMAEForVideoClassification, VideoMAEPreTrainedModel, Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "from typing import Optional, Tuple, Union\n",
    "from torch.nn import MSELoss, CrossEntropyLoss, BCEWithLogitsLoss\n",
    "import wandb\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = eval_pred.predictions\n",
    "    if isinstance(predictions, list):\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "    if len(predictions.shape) == 3:\n",
    "        predictions = predictions[:, :, 0]\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"video\"].permute(1, 0, 2, 3) for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "class CustomVideoMAE(VideoMAEPreTrainedModel):\n",
    "    def __init__(self, config, base_model, feature_extractor, label2id, id2label):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.videomae = base_model.videomae\n",
    "        \n",
    "        # Classifier head\n",
    "        self.fc_norm = nn.LayerNorm(config.hidden_size) if config.use_mean_pooling else None\n",
    "        self.feature_extractor = feature_extractor\n",
    "        out_size = 768\n",
    "        self.classifier = nn.Linear(\n",
    "            # uncomment below to use the original VideoMAE branch and concatenate features\n",
    "            # config.hidden_size + out_size,\n",
    "            out_size,\n",
    "            config.num_labels\n",
    "        )\n",
    "        self.conv1x1 = nn.Conv2d(4, 1, kernel_size=1)  # 1x1 convolution to reduce dimension\n",
    "        self.config.label2id = label2id\n",
    "        self.config.id2label = id2label\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        pixel_values: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, ImageClassifierOutput]:\n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        # uncomment below to use the original VideoMAE branch\n",
    "        # outputs = self.videomae(\n",
    "        #     pixel_values,\n",
    "        #     head_mask=head_mask,\n",
    "        #     output_attentions=output_attentions,\n",
    "        #     output_hidden_states=output_hidden_states,  # Ensure hidden states are returned\n",
    "        #     return_dict=return_dict,\n",
    "        # )\n",
    "        # if output_hidden_states:\n",
    "        #     sequence_output = outputs.hidden_states[-1]  # Ensure hidden states are returned\n",
    "        # else:\n",
    "        #     sequence_output = outputs[0]\n",
    "        # if self.fc_norm is not None:\n",
    "        #     sequence_output = self.fc_norm(sequence_output.mean(1))\n",
    "        # else:\n",
    "        #     sequence_output = sequence_output[:, 0]\n",
    "\n",
    "        frame_count = pixel_values.shape[1]\n",
    "        time_sample_interval = pixel_values.shape[1] // 4\n",
    "        features = []\n",
    "        for i in range(0, frame_count, time_sample_interval):\n",
    "            frame_features = self.feature_extractor(pixel_values[:, i, :, :, :])\n",
    "            features.append(frame_features)\n",
    "        features = torch.stack(features, dim=1)\n",
    "        features = self.conv1x1(features.unsqueeze(2)).squeeze(2)\n",
    "        features = features.squeeze(1)\n",
    "        # uncomment below to use the original VideoMAE branch and concatenate features\n",
    "        # concatenated_features = torch.cat((sequence_output, features), dim=1)\n",
    "        # logits = self.classifier(concatenated_features)\n",
    "        logits = self.classifier(features)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (logits,)  # + outputs[2:]  # Keep only logits\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            # hidden_states=outputs.hidden_states,\n",
    "            # attentions=outputs.attentions,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def get_features(self, pixel_values, feature_type=\"concatenated\"):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.videomae(pixel_values, output_hidden_states=True, return_dict=True)\n",
    "            sequence_output = outputs.hidden_states[-1][:, 0, :]  # CLS token representation\n",
    "\n",
    "            frame_count = pixel_values.shape[1]\n",
    "            time_sample_interval = pixel_values.shape[1] // 4\n",
    "            features = []\n",
    "            for i in range(0, frame_count, time_sample_interval):\n",
    "                frame_features = self.feature_extractor(pixel_values[:, i, :, :, :])\n",
    "                features.append(frame_features)\n",
    "            features = torch.stack(features, dim=1)\n",
    "            features = self.conv1x1(features.unsqueeze(2)).squeeze(2)\n",
    "            features = features.squeeze(1)\n",
    "\n",
    "            if feature_type == \"concatenated\":\n",
    "                return torch.cat((sequence_output, features), dim=1)\n",
    "            elif feature_type == \"TICL\":\n",
    "                return features\n",
    "            else:\n",
    "                raise ValueError(\"Invalid feature type. Choose either 'concatenated' or 'TICL'.\")\n",
    "\n",
    "\n",
    "\n",
    "base_model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "# Feature extractor\n",
    "time_feature = True\n",
    "pure_CLIP = False\n",
    "\n",
    "if time_feature == True:\n",
    "    if not pure_CLIP:\n",
    "        feature_extractor = TICL()\n",
    "        model_path = 'TICL_adapter_v1_best.pth'\n",
    "        feature_extractor.load_state_dict(torch.load(model_path))\n",
    "        feature_extractor = feature_extractor.image_encoder\n",
    "        feature_extractor.eval()\n",
    "    else:\n",
    "        feature_extractor = PureImageEncoder()\n",
    "        feature_extractor.eval()\n",
    "    custom_model = CustomVideoMAE(base_model.config, base_model, feature_extractor, label2id, id2label)\n",
    "else:\n",
    "    custom_model = base_model\n",
    "print(custom_model)\n",
    "# Training and evaluation\n",
    "batch_size = 2\n",
    "num_epochs = 20\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "if dataset == \"Hollywood2\":\n",
    "    run_name = \"hollywood2_videomae\"\n",
    "    # learning_rate = 1e-4\n",
    "    num_epochs = 20\n",
    "    learning_rate = 5e-5\n",
    "    warmup_ratio = 0.0\n",
    "elif dataset == \"YUP++\":\n",
    "    run_name = \"yup_videomae\"\n",
    "    learning_rate = 5e-5\n",
    "    warmup_ratio = 0.0\n",
    "    num_epochs = 10\n",
    "elif dataset == \"360x\":\n",
    "    batch_size = 2\n",
    "    run_name = \"360x_videomae\"\n",
    "    learning_rate = 7e-5\n",
    "    warmup_ratio = 0.0\n",
    "    num_epochs = 20\n",
    "else:\n",
    "    raise ValueError(f\"Unknown dataset: {dataset}\")\n",
    "time_feature_type = \"TICL\" if time_feature else \"None\"\n",
    "max_steps = (len(train_dataset) // batch_size) * num_epochs\n",
    "steps_per_epoch = len(train_dataset) // batch_size\n",
    "# add basic hyperparameters to the run name\n",
    "# run_name += f\"1:5val_TEST_resizeT_bs{batch_size}_epochs{num_epochs}_steps{max_steps}_lr{learning_rate}_wr{warmup_ratio}_Timefeature=4frame_{str(time_feature)}{time_feature_type}_third_view\"\n",
    "run_name += f\"1:5val_TEST_resizeT_bs{batch_size}_epochs{num_epochs}_steps{max_steps}_lr{learning_rate}_wr{warmup_ratio}_Timefeature=4frame_{str(time_feature)}{time_feature_type}_LP_third_view\"\n",
    "if pure_CLIP:\n",
    "    run_name += f\"_pureCLIP={str(pure_CLIP)}\"\n",
    "# 4 epochs one eval\n",
    "eval_steps = 1 * steps_per_epoch\n",
    "# print(eval_steps)\n",
    "out_dir = f\"./out/{dataset}/{run_name}\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    remove_unused_columns=False,\n",
    "    # strategies must be the same\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps = eval_steps,  # Save every 4 epochs as well\n",
    "    eval_steps=eval_steps,\n",
    "    save_total_limit=1,  # Only keep the 1 most recent checkpoints\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=num_epochs,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    max_steps=max_steps,\n",
    "    fp16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    custom_model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    # set evaluation interval to 1 to enable evaluation after each epoch\n",
    "    tokenizer=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "\n",
    "print(f\"////Run {run_name} preparation finished...////\")\n",
    "print(\"\"\"\\\n",
    "                              _- _ , - . _\n",
    "                            `,% o` ~~-_,'.'\n",
    "                            % %@ - % %, -'%,\n",
    "                           ,-, . _ --\\ -.%\n",
    "                  P^=.     `'\"   |+|'    `\n",
    "                  ||             |+|\n",
    "                  ||             |+|\n",
    "                  ||             |+|\n",
    "            ______/|             |+|\n",
    "           `| ___ ,/             |+|\n",
    "            ||   ||              |+|\n",
    "            ||   ||              |+|\n",
    "    ________||___||___.__________/H|____\n",
    "    ______               _          __             _____         _       _             _ \n",
    "    | ___ \\             | |        / _|           |_   _|       (_)     (_)           | |\n",
    "    | |_/ /___  __ _  __| |_   _  | |_ ___  _ __    | |_ __ __ _ _ _ __  _ _ __   __ _| |\n",
    "    |    // _ \\/ _` |/ _` | | | | |  _/ _ \\| '__|   | | '__/ _` | | '_ \\| | '_ \\ / _` | |\n",
    "    | |\\ \\  __/ (_| | (_| | |_| | | || (_) | |      | | | | (_| | | | | | | | | | (_| |_|\n",
    "    \\_| \\_\\___|\\__,_|\\__,_|\\__, | |_| \\___/|_|      \\_/_|  \\__,_|_|_| |_|_|_| |_|\\__, (_)\n",
    "                            __/ |                                                 __/ |  \n",
    "                           |___/                                                 |___/                                                                                                                                                                                                                                                              \n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m505029658\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/bask/homes/l/lindy/wandb/run-20241114_074450-czq3eacd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/505029658/huggingface/runs/czq3eacd' target=\"_blank\">360x_videomae1:5val_TEST_resizeT_bs2_epochs20_steps1520_lr7e-05_wr0.0_Timefeature=4frame_True_LP_third_view</a></strong> to <a href='https://wandb.ai/505029658/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/505029658/huggingface' target=\"_blank\">https://wandb.ai/505029658/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/505029658/huggingface/runs/czq3eacd' target=\"_blank\">https://wandb.ai/505029658/huggingface/runs/czq3eacd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1520' max='1520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1520/1520 1:13:27, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>3.226400</td>\n",
       "      <td>3.249602</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>3.115700</td>\n",
       "      <td>3.144314</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>2.981300</td>\n",
       "      <td>3.079608</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>2.682100</td>\n",
       "      <td>3.060203</td>\n",
       "      <td>0.148148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.760400</td>\n",
       "      <td>3.028357</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>2.349400</td>\n",
       "      <td>2.980650</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>2.741300</td>\n",
       "      <td>2.911820</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>2.454300</td>\n",
       "      <td>2.852078</td>\n",
       "      <td>0.296296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>2.392900</td>\n",
       "      <td>2.785821</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>1.490300</td>\n",
       "      <td>2.725021</td>\n",
       "      <td>0.351852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>2.218600</td>\n",
       "      <td>2.679132</td>\n",
       "      <td>0.351852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>1.976600</td>\n",
       "      <td>2.624149</td>\n",
       "      <td>0.351852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>2.358500</td>\n",
       "      <td>2.587569</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>1.412000</td>\n",
       "      <td>2.552352</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>1.665000</td>\n",
       "      <td>2.530386</td>\n",
       "      <td>0.370370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1216</td>\n",
       "      <td>1.894700</td>\n",
       "      <td>2.507542</td>\n",
       "      <td>0.425926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1292</td>\n",
       "      <td>1.334700</td>\n",
       "      <td>2.492545</td>\n",
       "      <td>0.425926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>1.682900</td>\n",
       "      <td>2.481253</td>\n",
       "      <td>0.425926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1444</td>\n",
       "      <td>1.938800</td>\n",
       "      <td>2.473864</td>\n",
       "      <td>0.425926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.763400</td>\n",
       "      <td>2.471184</td>\n",
       "      <td>0.425926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bask/homes/l/lindy/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27/27 00:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5075418949127197, 'eval_accuracy': 0.42592592592592593, 'eval_runtime': 53.5742, 'eval_samples_per_second': 1.008, 'eval_steps_per_second': 0.504, 'epoch': 20.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "# Log the final evaluation metrics\n",
    "print(\"testing model...\")\n",
    "\n",
    "final_metrics = trainer.evaluate(eval_dataset=test_dataset)\n",
    "wandb.log(final_metrics)\n",
    "print(final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img2img-turbo (Conda)",
   "language": "python",
   "name": "sys_img2img-turbo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
